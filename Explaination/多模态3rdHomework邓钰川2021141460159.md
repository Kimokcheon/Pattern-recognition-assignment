# 1
PCA是无监督的，也就是训练样本不需要标签；LDA是有监督的，也就是训练样本需要标签。PCA是去除掉原始数据中冗余的维度，而LDA是寻找一个维度，使得原始数据在该维度上投影后不同类别的数据尽可能分离开来。
相同：
1.  PCA和LDA都是经典的降维算法；
2.  PCA和LDA都假设数据是符合高斯分布的；
3.  PCA和LDA都利用了矩阵特征分解的思想。
不同：
1.  PCA是无监督（训练样本无标签）的，LDA是有监督（训练样本有标签）的；
6.  PCA是去掉原始数据冗余的维度，LDA是选择一个最佳的投影方向，使得投影后相同类别的数据分布紧凑，不同类别的数据尽量相互远离。
7.  LDA最多可以降到k-1维（k是训练样本的类别数量，k-1是因为最后一维的均值可以由前面的k-1维的均值表示）；
8.  LDA可能会过拟合数据
# 2
投影后类内方差最小，类间方差最大
# 3
- 两类样本投影到一维空间后的类内和类间 散度矩阵
$$
\begin{aligned}
&S_w^y=\sum_{y_i \in \omega_1}\left(y_i-\bar{y}_1\right)^2+\sum_{y_i \in \omega_2}\left(y_i-\bar{y}_2\right)^2 \\
&S_b^y=\left(\bar{y}_1-\bar{y}_2\right)^2
\end{aligned}
$$
将 $y=w^T x, \bar{y}_k=w^T m_k$ 代入得到
$$
\begin{aligned}
&S_w^y=w^T S_w w \\
&S_b^y=w^T S_b w=w^T\left(m_1-m_2\right)^2 w
\end{aligned}
$$
- Fisher线性判别准则
$$
w^*=\arg \max _w J_F(w)=\frac{w^T S_b w}{w^T S_w w}
$$
- 可以用拉格朗日乘子法求解, 两类问题投 影到一维空间的最优投影方向为
$$
w^*=S_w^{-1}\left(m_1-m_2\right)
$$
- 在投影后的一维空间中的分类规则
$$
x \in \begin{cases}\omega_1 & y=\left(w^*\right)^T x>y_0 \\ \omega_2 & y=\left(w^*\right)^T x<y_0\end{cases}
$$
# 4
GMA作为CCA的监督核化扩展，将不同模态空间中的数据映射到单个(非)线性子空间。上述方法虽然能够有效地发现所需的潜在表征，但由于得到的表征是公共空间中多模态数据的投影，没有明显的可解释性含义，往往缺乏直观的解释
$$
\underbrace{\arg \max }_{v_1, v_2}\left(v_1{ }^T A_1 v_1+\sum_{i=2}^n \mu_i v_i{ }^T A_i v_i+\sum_{i=1}^n \sum_{i \neq i}^n \lambda_{i j} v_1{ }^T Z_1 Z_2{ }^T v_2\right)
$$
