{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24812,"status":"ok","timestamp":1627619085236,"user":{"displayName":"Jagadeesh Nadimpalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiGtEyk2Ntf2Ry0318QvObdL70hIl4sqypFy_Z=s64","userId":"15245029944734342298"},"user_tz":-330},"id":"vDy7dkVBfMFt","outputId":"f12a97dc-b4d8-4253-cecd-09f489491733"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1627619089138,"user":{"displayName":"Jagadeesh Nadimpalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiGtEyk2Ntf2Ry0318QvObdL70hIl4sqypFy_Z=s64","userId":"15245029944734342298"},"user_tz":-330},"id":"QJi7kvqFfUCE","outputId":"5804473e-0865-4b19-a472-44f55ee9238b"},"outputs":[],"source":["%cd /content/drive/MyDrive/DF-GAN/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1627619089548,"user":{"displayName":"Jagadeesh Nadimpalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiGtEyk2Ntf2Ry0318QvObdL70hIl4sqypFy_Z=s64","userId":"15245029944734342298"},"user_tz":-330},"id":"ILwH0kpgfUFF","outputId":"bb8c1d0a-824a-4466-e9ef-f91cb467dc30"},"outputs":[],"source":["%cd code/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11342,"status":"ok","timestamp":1627619101357,"user":{"displayName":"Jagadeesh Nadimpalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiGtEyk2Ntf2Ry0318QvObdL70hIl4sqypFy_Z=s64","userId":"15245029944734342298"},"user_tz":-330},"id":"UmmRKdXWhKve","outputId":"12db1464-36b6-4707-9e85-e4c71ca60f5a"},"outputs":[],"source":["!pip install nltk\n","import nltk\n","nltk.download(\"book\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RtFKUtDz-G2k"},"source":["#**LIBRARIES**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wh_bRQkpfUHp"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.parallel # to run on multiple gpus\n","import torch.optim as optim #optimization algos(gradient descent)\n","import torch.backends.cudnn as cudnn\n","import torchvision.transforms as transforms\n","import torchvision.utils as ut\n","import torch.utils.data as data\n","import torch.utils.model_zoo as model_zoo\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import numpy.random as random\n","from PIL import Image\n","from torch.autograd import Variable\n","from torchvision import models\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from collections import OrderedDict\n","from collections import defaultdict\n","from nltk.tokenize import RegexpTokenizer\n","from easydict import EasyDict as edict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nuxfg01Yr_3H"},"outputs":[],"source":["import os\n","import errno\n","import numpy as np\n","from torch.nn import init\n","\n","import torch\n","import torch.nn as nn\n","\n","from PIL import Image, ImageDraw, ImageFont\n","from copy import deepcopy\n","import skimage.transform\n","\n","from miscc.config import cfg\n","\n","\n","# For visualization ################################################\n","COLOR_DIC = {0:[128,64,128],  1:[244, 35,232],\n","             2:[70, 70, 70],  3:[102,102,156],\n","             4:[190,153,153], 5:[153,153,153],\n","             6:[250,170, 30], 7:[220, 220, 0],\n","             8:[107,142, 35], 9:[152,251,152],\n","             10:[70,130,180], 11:[220,20, 60],\n","             12:[255, 0, 0],  13:[0, 0, 142],\n","             14:[119,11, 32], 15:[0, 60,100],\n","             16:[0, 80, 100], 17:[0, 0, 230],\n","             18:[0,  0, 70],  19:[0, 0,  0]}\n","FONT_MAX = 50\n","\n","\n","def drawCaption(convas, captions, ixtoword, vis_size, off1=2, off2=2):\n","    num = captions.size(0)\n","    img_txt = Image.fromarray(convas)\n","    # get a font\n","    # fnt = None  # ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n","    fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 50)\n","    # get a drawing context\n","    d = ImageDraw.Draw(img_txt)\n","    sentence_list = []\n","    for i in range(num):\n","        cap = captions[i].data.cpu().numpy()\n","        sentence = []\n","        for j in range(len(cap)):\n","            if cap[j] == 0:\n","                break\n","            word = ixtoword[cap[j]].encode('ascii', 'ignore').decode('ascii')\n","            d.text(((j + off1) * (vis_size + off2), i * FONT_MAX), '%d:%s' % (j, word[:6]),\n","                   font=fnt, fill=(255, 255, 255, 255))\n","            sentence.append(word)\n","        sentence_list.append(sentence)\n","    return img_txt, sentence_list\n","\n","\n","def build_super_images(real_imgs, captions, ixtoword,\n","                       attn_maps, att_sze, lr_imgs=None,\n","                       batch_size=cfg.TRAIN.BATCH_SIZE,\n","                       max_word_num=cfg.TEXT.WORDS_NUM):\n","    nvis = 8\n","    real_imgs = real_imgs[:nvis]\n","    if lr_imgs is not None:\n","        lr_imgs = lr_imgs[:nvis]\n","    if att_sze == 17:\n","        vis_size = att_sze * 16\n","    else:\n","        vis_size = real_imgs.size(2)\n","\n","    text_convas = \\\n","        np.ones([batch_size * FONT_MAX,\n","                 (max_word_num + 2) * (vis_size + 2), 3],\n","                dtype=np.uint8)\n","\n","    for i in range(max_word_num):\n","        istart = (i + 2) * (vis_size + 2)\n","        iend = (i + 3) * (vis_size + 2)\n","        text_convas[:, istart:iend, :] = COLOR_DIC[i]\n","\n","\n","    real_imgs = \\\n","        nn.functional.interpolate(real_imgs,size=(vis_size, vis_size),\n","                                  mode='bilinear', align_corners=False)\n","    # [-1, 1] --> [0, 1]\n","    real_imgs.add_(1).div_(2).mul_(255)\n","    real_imgs = real_imgs.data.numpy()\n","    # b x c x h x w --> b x h x w x c\n","    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n","    pad_sze = real_imgs.shape\n","    middle_pad = np.zeros([pad_sze[2], 2, 3])\n","    post_pad = np.zeros([pad_sze[1], pad_sze[2], 3])\n","    if lr_imgs is not None:\n","        lr_imgs = \\\n","            nn.functional.interpolate(lr_imgs,size=(vis_size, vis_size),\n","                                  mode='bilinear', align_corners=False)\n","        # [-1, 1] --> [0, 1]\n","        lr_imgs.add_(1).div_(2).mul_(255)\n","        lr_imgs = lr_imgs.data.numpy()\n","        # b x c x h x w --> b x h x w x c\n","        lr_imgs = np.transpose(lr_imgs, (0, 2, 3, 1))\n","\n","    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n","    seq_len = max_word_num\n","    img_set = []\n","    num = nvis  # len(attn_maps)\n","\n","    text_map, sentences = \\\n","        drawCaption(text_convas, captions, ixtoword, vis_size)\n","    text_map = np.asarray(text_map).astype(np.uint8)\n","\n","    bUpdate = 1\n","    for i in range(num):\n","        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n","        # --> 1 x 1 x 17 x 17\n","        attn_max = attn.max(dim=1, keepdim=True)\n","        attn = torch.cat([attn_max[0], attn], 1)\n","        #\n","        attn = attn.view(-1, 1, att_sze, att_sze)\n","        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n","        # n x c x h x w --> n x h x w x c\n","        attn = np.transpose(attn, (0, 2, 3, 1))\n","        num_attn = attn.shape[0]\n","        #\n","        img = real_imgs[i]\n","        if lr_imgs is None:\n","            lrI = img\n","        else:\n","            lrI = lr_imgs[i]\n","        row = [lrI, middle_pad]\n","        row_merge = [img, middle_pad]\n","        row_beforeNorm = []\n","        minVglobal, maxVglobal = 1, 0\n","        for j in range(num_attn):\n","            one_map = attn[j]\n","            if (vis_size // att_sze) > 1:\n","                one_map = \\\n","                    skimage.transform.pyramid_expand(one_map, sigma=20,\n","                                                     upscale=vis_size // att_sze,\n","                                                     multichannel=True)\n","            row_beforeNorm.append(one_map)\n","            minV = one_map.min()\n","            maxV = one_map.max()\n","            if minVglobal > minV:\n","                minVglobal = minV\n","            if maxVglobal < maxV:\n","                maxVglobal = maxV\n","        for j in range(seq_len + 1):\n","            if j < num_attn:\n","                one_map = row_beforeNorm[j]\n","                one_map = (one_map - minVglobal) / (maxVglobal - minVglobal)\n","                one_map *= 255\n","                #\n","                PIL_im = Image.fromarray(np.uint8(img))\n","                PIL_att = Image.fromarray(np.uint8(one_map))\n","                merged = \\\n","                    Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n","                mask = Image.new('L', (vis_size, vis_size), (210))\n","                merged.paste(PIL_im, (0, 0))\n","                merged.paste(PIL_att, (0, 0), mask)\n","                merged = np.array(merged)[:, :, :3]\n","            else:\n","                one_map = post_pad\n","                merged = post_pad\n","            row.append(one_map)\n","            row.append(middle_pad)\n","            #\n","            row_merge.append(merged)\n","            row_merge.append(middle_pad)\n","        row = np.concatenate(row, 1)\n","        row_merge = np.concatenate(row_merge, 1)\n","        txt = text_map[i * FONT_MAX: (i + 1) * FONT_MAX]\n","        if txt.shape[1] != row.shape[1]:\n","            print('txt', txt.shape, 'row', row.shape)\n","            bUpdate = 0\n","            break\n","        row = np.concatenate([txt, row, row_merge], 0)\n","        img_set.append(row)\n","    if bUpdate:\n","        img_set = np.concatenate(img_set, 0)\n","        img_set = img_set.astype(np.uint8)\n","        return img_set, sentences\n","    else:\n","        return None\n","\n","\n","def build_super_images2(real_imgs, captions, cap_lens, ixtoword,\n","                        attn_maps, att_sze, vis_size=256, topK=5):\n","    batch_size = real_imgs.size(0)\n","    max_word_num = np.max(cap_lens)\n","    text_convas = np.ones([batch_size * FONT_MAX,\n","                           max_word_num * (vis_size + 2), 3],\n","                           dtype=np.uint8)\n","\n","    real_imgs = \\\n","        nn.functional.interpolate(real_imgs,size=(vis_size, vis_size),\n","                                    mode='bilinear', align_corners=False)\n","    # [-1, 1] --> [0, 1]\n","    real_imgs.add_(1).div_(2).mul_(255)\n","    real_imgs = real_imgs.data.numpy()\n","    # b x c x h x w --> b x h x w x c\n","    real_imgs = np.transpose(real_imgs, (0, 2, 3, 1))\n","    pad_sze = real_imgs.shape\n","    middle_pad = np.zeros([pad_sze[2], 2, 3])\n","\n","    # batch x seq_len x 17 x 17 --> batch x 1 x 17 x 17\n","    img_set = []\n","    num = len(attn_maps)\n","\n","    text_map, sentences = \\\n","        drawCaption(text_convas, captions, ixtoword, vis_size, off1=0)\n","    text_map = np.asarray(text_map).astype(np.uint8)\n","\n","    bUpdate = 1\n","    for i in range(num):\n","        attn = attn_maps[i].cpu().view(1, -1, att_sze, att_sze)\n","        #\n","        attn = attn.view(-1, 1, att_sze, att_sze)\n","        attn = attn.repeat(1, 3, 1, 1).data.numpy()\n","        # n x c x h x w --> n x h x w x c\n","        attn = np.transpose(attn, (0, 2, 3, 1))\n","        num_attn = cap_lens[i]\n","        thresh = 2./float(num_attn)\n","        #\n","        img = real_imgs[i]\n","        row = []\n","        row_merge = []\n","        row_txt = []\n","        row_beforeNorm = []\n","        conf_score = []\n","        for j in range(num_attn):\n","            one_map = attn[j]\n","            mask0 = one_map > (2. * thresh)\n","            conf_score.append(np.sum(one_map * mask0))\n","            mask = one_map > thresh\n","            one_map = one_map * mask\n","            if (vis_size // att_sze) > 1:\n","                one_map = \\\n","                    skimage.transform.pyramid_expand(one_map, sigma=20,\n","                                                     upscale=vis_size // att_sze,\n","                                                     multichannel=True)\n","            minV = one_map.min()\n","            maxV = one_map.max()\n","            one_map = (one_map - minV) / (maxV - minV)\n","            row_beforeNorm.append(one_map)\n","        sorted_indices = np.argsort(conf_score)[::-1]\n","\n","        for j in range(num_attn):\n","            one_map = row_beforeNorm[j]\n","            one_map *= 255\n","            #\n","            PIL_im = Image.fromarray(np.uint8(img))\n","            PIL_att = Image.fromarray(np.uint8(one_map))\n","            merged = \\\n","                Image.new('RGBA', (vis_size, vis_size), (0, 0, 0, 0))\n","            mask = Image.new('L', (vis_size, vis_size), (180))  # (210)\n","            merged.paste(PIL_im, (0, 0))\n","            merged.paste(PIL_att, (0, 0), mask)\n","            merged = np.array(merged)[:, :, :3]\n","\n","            row.append(np.concatenate([one_map, middle_pad], 1))\n","            #\n","            row_merge.append(np.concatenate([merged, middle_pad], 1))\n","            #\n","            txt = text_map[i * FONT_MAX:(i + 1) * FONT_MAX,\n","                           j * (vis_size + 2):(j + 1) * (vis_size + 2), :]\n","            row_txt.append(txt)\n","        # reorder\n","        row_new = []\n","        row_merge_new = []\n","        txt_new = []\n","        for j in range(num_attn):\n","            idx = sorted_indices[j]\n","            row_new.append(row[idx])\n","            row_merge_new.append(row_merge[idx])\n","            txt_new.append(row_txt[idx])\n","        row = np.concatenate(row_new[:topK], 1)\n","        row_merge = np.concatenate(row_merge_new[:topK], 1)\n","        txt = np.concatenate(txt_new[:topK], 1)\n","        if txt.shape[1] != row.shape[1]:\n","            print('Warnings: txt', txt.shape, 'row', row.shape,\n","                  'row_merge_new', row_merge_new.shape)\n","            bUpdate = 0\n","            break\n","        row = np.concatenate([txt, row_merge], 0)\n","        img_set.append(row)\n","    if bUpdate:\n","        img_set = np.concatenate(img_set, 0)\n","        img_set = img_set.astype(np.uint8)\n","        return img_set, sentences\n","    else:\n","        return None\n","\n","\n","####################################################################\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.orthogonal_(m.weight.data, 1.0)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","    elif classname.find('Linear') != -1:\n","        nn.init.orthogonal_(m.weight.data, 1.0)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0.0)\n","\n","\n","def load_params(model, new_param):\n","    for p, new_p in zip(model.parameters(), new_param):\n","        p.data.copy_(new_p)\n","\n","\n","def copy_G_params(model):\n","    flatten = deepcopy(list(p.data for p in model.parameters()))\n","    return flatten\n","\n","def mkdir_p(path):\n","    try:\n","        os.makedirs(path)\n","    except OSError as exc:  # Python >2.5\n","        if exc.errno == errno.EEXIST and os.path.isdir(path):\n","            pass\n","        else:\n","            raise\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfyVKmB7fW1R"},"outputs":[],"source":["cfg = edict()\n","# Dataset name: flowers, birds\n","cfg.DATASET_NAME = 'birds'\n","cfg.CONFIG_NAME = 'bird'\n","cfg.DATA_DIR = '/content/drive/MyDrive/DF-GAN/data/birds'\n","cfg.GPU_ID = 0\n","cfg.CUDA = True\n","cfg.WORKERS = 6\n","\n","cfg.RNN_TYPE = 'LSTM'   # 'GRU'\n","cfg.B_VALIDATION = True\n","cfg.loss = 'hinge'\n","cfg.TREE = edict()\n","cfg.TREE.BRANCH_NUM = 1\n","cfg.TREE.BASE_SIZE =256\n","\n","\n","# Training options\n","cfg.TRAIN = edict()\n","cfg.TRAIN.BATCH_SIZE = 24\n","cfg.TRAIN.MAX_EPOCH = 601\n","cfg.TRAIN.SNAPSHOT_INTERVAL = 2000\n","cfg.TRAIN.DISCRIMINATOR_LR = 2e-4\n","cfg.TRAIN.GENERATOR_LR = 2e-4\n","cfg.TRAIN.ENCODER_LR = 2e-4\n","cfg.TRAIN.RNN_GRAD_CLIP = 0.25\n","cfg.TRAIN.FLAG = True\n","cfg.TRAIN.NET_E = ''\n","cfg.TRAIN.NET_G ='/content/drive/MyDrive/DF-GAN/'\n","cfg.TRAIN.B_NET_D = True\n","cfg.TRAIN.NF = 32\n","cfg.TRAIN.SMOOTH = edict()\n","cfg.TRAIN.SMOOTH.GAMMA1 = 5.0\n","cfg.TRAIN.SMOOTH.GAMMA3 = 10.0\n","cfg.TRAIN.SMOOTH.GAMMA2 = 5.0\n","cfg.TRAIN.SMOOTH.LAMBDA = 1.0\n","\n","\n","# Modal options\n","cfg.GAN = edict()\n","cfg.GAN.DF_DIM = 64\n","cfg.GAN.GF_DIM = 128\n","cfg.GAN.Z_DIM = 100\n","cfg.GAN.CONDITION_DIM = 100\n","cfg.GAN.R_NUM = 2\n","cfg.GAN.B_ATTENTION = True\n","cfg.GAN.B_DCGAN = True\n","\n","\n","cfg.TEXT = edict()\n","cfg.TEXT.CAPTIONS_PER_IMAGE = 10\n","cfg.TEXT.EMBEDDING_DIM = 256\n","cfg.TEXT.WORDS_NUM = 18\n","cfg.TEXT.DAMSM_NAME = '/content/drive/MyDrive/DF-GAN/DAMSMencoders/bird/inception/text_encoder200.pth'\n","\n","def _merge_a_into_b(a, b):\n","    \"\"\"Merge config dictionary a into config dictionary b, clobbering the\n","    options in b whenever they are also specified in a.\n","    \"\"\"\n","    if type(a) is not edict:\n","        return\n","\n","    for k, v in a.items():\n","        # a must specify keys that are in b\n","        if k not in b:\n","            raise KeyError('{} is not a valid config key'.format(k))\n","\n","        # the types must match, too\n","        old_type = type(b[k])\n","        if old_type is not type(v):\n","            if isinstance(b[k], np.ndarray):\n","                v = np.array(v, dtype=b[k].dtype)\n","            else:\n","                raise ValueError(('Type mismatch ({} vs. {}) '\n","                                  'for config key: {}').format(type(b[k]),\n","                                                               type(v), k))\n","\n","        # recursively merge dicts\n","        if type(v) is edict:\n","            try:\n","                _merge_a_into_b(a[k], b[k])\n","            except:\n","                print('Error under config key: {}'.format(k))\n","                raise\n","        else:\n","            b[k] = v\n","\n","\n","def cfg_from_file(filename):\n","    \"\"\"Load a config file and merge it into the default options.\"\"\"\n","    import yaml\n","    with open(filename, 'r') as f:\n","        yaml_cfg = edict(yaml.load(f))\n","\n","    _merge_a_into_b(yaml_cfg, cfg)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wJXI64VP7k8Q"},"source":["#**RNNENCODER**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IW5DGhklfW6O"},"outputs":[],"source":["# ############## Text2Image Encoder-Decoder #######\n","class RNN_ENCODER(nn.Module):\n","    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n","                 nhidden=128, nlayers=1, bidirectional=True):\n","        super(RNN_ENCODER, self).__init__()\n","        self.n_steps = cfg.TEXT.WORDS_NUM\n","        self.ntoken = ntoken  # size of the dictionary\n","        self.ninput = ninput  # size of each embedding vector\n","        self.drop_prob = drop_prob  # probability of an element to be zeroed\n","        self.nlayers = nlayers  # Number of recurrent layers\n","        self.bidirectional = bidirectional\n","        self.rnn_type = cfg.RNN_TYPE\n","        if bidirectional:\n","            self.num_directions = 2\n","        else:\n","            self.num_directions = 1\n","        # number of features in the hidden state\n","        self.nhidden = nhidden // self.num_directions\n","\n","        self.define_module()\n","        self.init_weights()\n","\n","    def define_module(self):\n","        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n","        self.drop = nn.Dropout(self.drop_prob)\n","        if self.rnn_type == 'LSTM':\n","            # dropout: If non-zero, introduces a dropout layer on\n","            # the outputs of each RNN layer except the last layer\n","            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n","                               self.nlayers, batch_first=True,\n","                               dropout=self.drop_prob,\n","                               bidirectional=self.bidirectional)\n","        elif self.rnn_type == 'GRU':\n","            self.rnn = nn.GRU(self.ninput, self.nhidden,\n","                              self.nlayers, batch_first=True,\n","                              dropout=self.drop_prob,\n","                              bidirectional=self.bidirectional)\n","        else:\n","            raise NotImplementedError\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        # Do not need to initialize RNN parameters, which have been initialized\n","        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n","        # self.decoder.weight.data.uniform_(-initrange, initrange)\n","        # self.decoder.bias.data.fill_(0)\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters()).data\n","        if self.rnn_type == 'LSTM':\n","            return (Variable(weight.new(self.nlayers * self.num_directions,\n","                                        bsz, self.nhidden).zero_()),\n","                    Variable(weight.new(self.nlayers * self.num_directions,\n","                                        bsz, self.nhidden).zero_()))\n","        else:\n","            return Variable(weight.new(self.nlayers * self.num_directions,\n","                                       bsz, self.nhidden).zero_())\n","\n","    def forward(self, captions, cap_lens, hidden, mask=None):\n","        # input: torch.LongTensor of size batch x n_steps\n","        # --> emb: batch x n_steps x ninput\n","        emb = self.drop(self.encoder(captions))\n","        #\n","        # Returns: a PackedSequence object\n","        cap_lens = cap_lens.data.tolist()\n","        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n","        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n","        # tensor containing the initial hidden state for each element in batch.\n","        # #output (batch, seq_len, hidden_size * num_directions)\n","        # #or a PackedSequence object:\n","        # tensor containing output features (h_t) from the last layer of RNN\n","        output, hidden = self.rnn(emb, hidden)\n","        # PackedSequence object\n","        # --> (batch, seq_len, hidden_size * num_directions)\n","        output = pad_packed_sequence(output, batch_first=True)[0]\n","        # output = self.drop(output)\n","        # --> batch x hidden_size*num_directions x seq_len\n","        words_emb = output.transpose(1, 2)\n","        # --> batch x num_directions*hidden_size\n","        if self.rnn_type == 'LSTM':\n","            sent_emb = hidden[0].transpose(0, 1).contiguous()\n","        else:\n","            sent_emb = hidden.transpose(0, 1).contiguous()\n","        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n","        return words_emb, sent_emb"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yDRIrfTC9332"},"source":["#**PREPARE DATA FOR TRAINING AND TESTING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBfAIhG-fW88"},"outputs":[],"source":["import sys\n","if sys.version_info[0] == 2:\n","    import cPickle as pickle\n","else:\n","    import pickle\n","\n","def prepare_data(data):\n","    imgs, captions, captions_lens, class_ids, keys = data\n","\n","    # sort data by the length in a decreasing order\n","    #print(\"captiosn_lesn\",len(captions_lens))\n","    #print(\"captions\",len(captions))\n","    #print(\"captions\",captions)\n","    #print(\"caplens\",captions_lens)\n","    sorted_cap_lens, sorted_cap_indices = \\\n","        torch.sort(captions_lens, 0, True)\n","\n","    real_imgs = []\n","    for i in range(len(imgs)):\n","        imgs[i] = imgs[i][sorted_cap_indices]\n","        if cfg.CUDA:\n","            real_imgs.append(Variable(imgs[i]).cuda())\n","        else:\n","            real_imgs.append(Variable(imgs[i]))\n","\n","    captions = captions[sorted_cap_indices].squeeze()\n","    class_ids = class_ids[sorted_cap_indices].numpy()\n","    # sent_indices = sent_indices[sorted_cap_indices]\n","    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n","    # print('keys', type(keys), keys[-1])  # list\n","    if cfg.CUDA:\n","        captions = Variable(captions).cuda()\n","        sorted_cap_lens = Variable(sorted_cap_lens).cuda()\n","    else:\n","        captions = Variable(captions)\n","        sorted_cap_lens = Variable(sorted_cap_lens)\n","\n","    return [real_imgs, captions, sorted_cap_lens,\n","            class_ids, keys]\n","\n","\n","def get_imgs(img_path, imsize, bbox=None,\n","             transform=None, normalize=None):\n","    img = Image.open(img_path).convert('RGB')\n","    width, height = img.size\n","    if bbox is not None:\n","        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n","        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n","        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n","        y1 = np.maximum(0, center_y - r)\n","        y2 = np.minimum(height, center_y + r)\n","        x1 = np.maximum(0, center_x - r)\n","        x2 = np.minimum(width, center_x + r)\n","        img = img.crop([x1, y1, x2, y2])\n","\n","    if transform is not None:\n","        img = transform(img)\n","        \n","    ret = []\n","    ret.append(normalize(img))\n","    #if cfg.GAN.B_DCGAN:\n","    '''\n","    for i in range(cfg.TREE.BRANCH_NUM):\n","        # print(imsize[i])\n","        re_img = transforms.Resize(imsize[i])(img)\n","        ret.append(normalize(re_img))\n","    '''\n","\n","    return ret\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"87ulBBHT9wCs"},"source":["#**LOAD TEXT DATA**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFK0Z1sR9s6-"},"outputs":[],"source":["class TextDataset(data.Dataset):\n","    def __init__(self, data_dir, split='train',\n","                 base_size=64,\n","                 transform=None, target_transform=None):\n","        self.transform = transform\n","        self.norm = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","        self.target_transform = target_transform\n","        self.embeddings_num = cfg.TEXT.CAPTIONS_PER_IMAGE\n","\n","        self.imsize = []\n","        for i in range(cfg.TREE.BRANCH_NUM):\n","            self.imsize.append(base_size)\n","            base_size = base_size * 2\n","\n","        self.data = []\n","        self.data_dir = data_dir\n","        if data_dir.find('birds') != -1:\n","            self.bbox = self.load_bbox()\n","        else:\n","            self.bbox = None\n","        split_dir = os.path.join(data_dir, split)\n","\n","        self.filenames, self.captions, self.ixtoword, \\\n","            self.wordtoix, self.n_words = self.load_text_data(data_dir, split)\n","\n","        self.class_id = self.load_class_id(split_dir, len(self.filenames))\n","        self.number_example = len(self.filenames)\n","\n","    def load_bbox(self):\n","        data_dir = self.data_dir\n","        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n","        df_bounding_boxes = pd.read_csv(bbox_path,\n","                                        delim_whitespace=True,\n","                                        header=None).astype(int)\n","        #\n","        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n","        df_filenames = \\\n","            pd.read_csv(filepath, delim_whitespace=True, header=None)\n","        filenames = df_filenames[1].tolist()\n","        print('Total filenames: ', len(filenames), filenames[0])\n","        #\n","        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n","        numImgs = len(filenames)\n","        for i in range(0, numImgs):\n","            # bbox = [x-left, y-top, width, height]\n","            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n","\n","            key = filenames[i][:-4]\n","            filename_bbox[key] = bbox\n","        #\n","        return filename_bbox\n","\n","    def load_captions(self, data_dir, filenames):\n","        all_captions = []\n","        for i in range(len(filenames)):\n","            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n","            with open(cap_path, \"r\") as f:\n","                captions = f.read().decode('utf8').split('\\n')\n","                cnt = 0\n","                for cap in captions:\n","                    if len(cap) == 0:\n","                        continue\n","                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n","                    # picks out sequences of alphanumeric characters as tokens\n","                    # and drops everything else\n","                    tokenizer = RegexpTokenizer(r'\\w+')\n","                    tokens = tokenizer.tokenize(cap.lower())\n","                    # print('tokens', tokens)\n","                    if len(tokens) == 0:\n","                        print('cap', cap)\n","                        continue\n","\n","                    tokens_new = []\n","                    for t in tokens:\n","                        t = t.encode('ascii', 'ignore').decode('ascii')\n","                        if len(t) > 0:\n","                            tokens_new.append(t)\n","                    all_captions.append(tokens_new)\n","                    cnt += 1\n","                    if cnt == self.embeddings_num:\n","                        break\n","                if cnt < self.embeddings_num:\n","                    print('ERROR: the captions for %s less than %d'\n","                          % (filenames[i], cnt))\n","        return all_captions\n","\n","    def build_dictionary(self, train_captions, test_captions):\n","        word_counts = defaultdict(float)\n","        captions = train_captions + test_captions\n","        for sent in captions:\n","            for word in sent:\n","                word_counts[word] += 1\n","\n","        vocab = [w for w in word_counts if word_counts[w] >= 0]\n","\n","        ixtoword = {}\n","        ixtoword[0] = '<end>'\n","        wordtoix = {}\n","        wordtoix['<end>'] = 0\n","        ix = 1\n","        for w in vocab:\n","            wordtoix[w] = ix\n","            ixtoword[ix] = w\n","            ix += 1\n","\n","        train_captions_new = []\n","        for t in train_captions:\n","            rev = []\n","            for w in t:\n","                if w in wordtoix:\n","                    rev.append(wordtoix[w])\n","            # rev.append(0)  # do not need '<end>' token\n","            train_captions_new.append(rev)\n","\n","        test_captions_new = []\n","        for t in test_captions:\n","            rev = []\n","            for w in t:\n","                if w in wordtoix:\n","                    rev.append(wordtoix[w])\n","            # rev.append(0)  # do not need '<end>' token\n","            test_captions_new.append(rev)\n","\n","        return [train_captions_new, test_captions_new,\n","                ixtoword, wordtoix, len(ixtoword)]\n","\n","    def load_text_data(self, data_dir, split):\n","        filepath = os.path.join(data_dir, 'captions.pickle')\n","        train_names = self.load_filenames(data_dir, 'train')\n","        test_names = self.load_filenames(data_dir, 'test')\n","        if not os.path.isfile(filepath):\n","            train_captions = self.load_captions(data_dir, train_names)\n","            test_captions = self.load_captions(data_dir, test_names)\n","\n","            train_captions, test_captions, ixtoword, wordtoix, n_words = \\\n","                self.build_dictionary(train_captions, test_captions)\n","            with open(filepath, 'wb') as f:\n","                pickle.dump([train_captions, test_captions,\n","                             ixtoword, wordtoix], f, protocol=2)\n","                print('Save to: ', filepath)\n","        else:\n","            with open(filepath, 'rb') as f:\n","                x = pickle.load(f)\n","                train_captions, test_captions = x[0], x[1]\n","                ixtoword, wordtoix = x[2], x[3]\n","                del x\n","                n_words = len(ixtoword)\n","                print('Load from: ', filepath)\n","        if split == 'train':\n","            # a list of list: each list contains\n","            # the indices of words in a sentence\n","            captions = train_captions\n","            filenames = train_names\n","        else:  # split=='test'\n","            captions = test_captions\n","            filenames = test_names\n","        return filenames, captions, ixtoword, wordtoix, n_words\n","\n","    def load_class_id(self, data_dir, total_num):\n","        if os.path.isfile(data_dir + '/class_info.pickle'):\n","            with open(data_dir + '/class_info.pickle', 'rb') as f:\n","                class_id = pickle.load(f, encoding=\"bytes\")\n","        else:\n","            class_id = np.arange(total_num)\n","        return class_id\n","\n","    def load_filenames(self, data_dir, split):\n","        filepath = '%s/%s/filenames.pickle' % (data_dir, split)\n","        if os.path.isfile(filepath):\n","            with open(filepath, 'rb') as f:\n","                filenames = pickle.load(f)\n","            print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n","        else:\n","            filenames = []\n","        return filenames\n","\n","    def get_caption(self, sent_ix):\n","        # a list of indices for a sentence\n","        sent_caption = np.asarray(self.captions[sent_ix]).astype('int64')\n","        if (sent_caption == 0).sum() > 0:\n","            print('ERROR: do not need END (0) token', sent_caption)\n","        num_words = len(sent_caption)\n","        # pad with 0s (i.e., '<end>')\n","        x = np.zeros((cfg.TEXT.WORDS_NUM, 1), dtype='int64')\n","        x_len = num_words\n","        if num_words <= cfg.TEXT.WORDS_NUM:\n","            x[:num_words, 0] = sent_caption\n","        else:\n","            ix = list(np.arange(num_words))  # 1, 2, 3,..., maxNum\n","            np.random.shuffle(ix)\n","            ix = ix[:cfg.TEXT.WORDS_NUM]\n","            ix = np.sort(ix)\n","            x[:, 0] = sent_caption[ix]\n","            x_len = cfg.TEXT.WORDS_NUM\n","        return x, x_len\n","\n","    def __getitem__(self, index):\n","        #\n","        key = self.filenames[index]\n","        cls_id = self.class_id[index]\n","        #\n","        if self.bbox is not None:\n","            bbox = self.bbox[key]\n","            data_dir = '%s/CUB_200_2011' % self.data_dir\n","        else:\n","            bbox = None\n","            data_dir = self.data_dir\n","        #\n","        img_name = '%s/images/%s.jpg' % (data_dir, key)\n","        imgs = get_imgs(img_name, self.imsize,\n","                        bbox, self.transform, normalize=self.norm)\n","        # random select a sentence\n","        sent_ix = random.randint(0, self.embeddings_num)\n","        new_sent_ix = index * self.embeddings_num + sent_ix\n","        caps, cap_len = self.get_caption(new_sent_ix)\n","        return imgs, caps, cap_len, cls_id, key\n","\n","\n","    def __len__(self):\n","        return len(self.filenames)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K8VEC-0t70-r"},"source":["#**GENERATOR**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAb-RweKpABZ"},"outputs":[],"source":["class Generator(nn.Module):\n","    def __init__(self, ngf=64, nz=100):\n","        super(Generator, self).__init__()\n","        self.ngf = ngf\n","\n","        # layer1 The input is a 100x1x1 random noise, and the output size is (ngf*8)x4x4\n","        self.fc = nn.Linear(nz, ngf*8*4*4)\n","        self.block0 = UpBlock(ngf * 8, ngf * 8)#4x4\n","        self.block1 = UpBlock(ngf * 8, ngf * 8)#4x4\n","        self.block2 = UpBlock(ngf * 8, ngf * 8)#8x8\n","        self.block3 = UpBlock(ngf * 8, ngf * 8)#16x16\n","        self.block4 = UpBlock(ngf * 8, ngf * 4)#32x32\n","        self.block5 = UpBlock(ngf * 4, ngf * 2)#64x64\n","        self.block6 = UpBlock(ngf * 2, ngf * 1)#128x128\n","\n","        self.conv_img = nn.Sequential(\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Conv2d(ngf, 3, 3, 1, 1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x, c):\n","\n","        out = self.fc(x)\n","        out = out.view(x.size(0), 8*self.ngf, 4, 4)\n","        out = self.block0(out,c)\n","\n","        out = F.interpolate(out, scale_factor=2)\n","        out = self.Upblock1(out,c)\n","\n","        out = F.interpolate(out, scale_factor=2)\n","        out = self.block2(out,c)\n","\n","        out = F.interpolate(out, scale_factor=2)\n","        out = self.block3(out,c)\n","\n","        out = F.interpolate(out, scale_factor=2)\n","        out = self.block4(out,c)\n","\n","        out = F.interpolate(out, scale_factor=2)\n","        out = self.block5(out,c)\n","\n","        out = F.interpolate(out, scale_factor=2)\n","        out = self.block6(out,c)\n","\n","        out = self.conv_img(out)\n","\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_9QVFmiE8QYN"},"source":["#**UPBLOCK**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yX8eYEbc782S"},"outputs":[],"source":["class UpBlock(nn.Module):\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(UpBlock, self).__init__()\n","\n","        self.learnable_sc = in_ch != out_ch \n","        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n","        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n","        self.affine0 = Affine(in_ch)\n","        self.affine1 = Affine(in_ch)\n","        self.affine2 = Affine(out_ch)\n","        self.affine3 = Affine(out_ch)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","        if self.learnable_sc:\n","            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n","\n","    def forward(self, x, y=None):\n","        return self.shortcut(x) + self.gamma * self.residual(x, y)\n","\n","    def shortcut(self, x):\n","        if self.learnable_sc:\n","            x = self.c_sc(x)\n","        return x\n","\n","    def residual(self, x, y=None):\n","        h = self.affine0(x, y)\n","        h = nn.LeakyReLU(0.2,inplace=True)(h)\n","        h = self.affine1(h, y)\n","        h = nn.LeakyReLU(0.2,inplace=True)(h)\n","        h = self.c1(h)\n","        \n","        h = self.affine2(h, y)\n","        h = nn.LeakyReLU(0.2,inplace=True)(h)\n","        h = self.affine3(h, y)\n","        h = nn.LeakyReLU(0.2,inplace=True)(h)\n","        return self.c2(h)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vVuGtE8p8UTa"},"source":["#**AFFINE TRANSFORMATION**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-G89-rN8B5q"},"outputs":[],"source":["class Affine(nn.Module):\n","\n","    def __init__(self, num_features):\n","        super(Affine, self).__init__()\n","\n","        self.fc_gamma = nn.Sequential(OrderedDict([\n","            ('linear1',nn.Linear(256, 256)),\n","            ('relu1',nn.ReLU(inplace=True)),\n","            ('linear2',nn.Linear(256, num_features)),\n","            ]))\n","        self.fc_beta = nn.Sequential(OrderedDict([\n","            ('linear1',nn.Linear(256, 256)),\n","            ('relu1',nn.ReLU(inplace=True)),\n","            ('linear2',nn.Linear(256, num_features)),\n","            ]))\n","        self._initialize()\n","\n","    def _initialize(self):\n","        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n","        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n","        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n","        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n","\n","    def forward(self, x, y=None):\n","\n","        weight = self.fc_gamma(y)\n","        bias = self.fc_beta(y)        \n","\n","        if weight.dim() == 1:\n","            weight = weight.unsqueeze(0)\n","        if bias.dim() == 1:\n","            bias = bias.unsqueeze(0)\n","\n","        size = x.size()\n","        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n","        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n","        return weight * x + bias\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HCVlk6qb8oM1"},"source":["#**CONCAT IMAGE FEATURES AND TEXT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfKZoMGF8Gdv"},"outputs":[],"source":["class D_GET_LOGITS(nn.Module):\n","    def __init__(self, ndf):\n","        super(D_GET_LOGITS, self).__init__()\n","        self.df_dim = ndf\n","\n","        self.joint_conv = nn.Sequential(\n","            nn.Conv2d(ndf * 16+256, ndf * 2, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2,inplace=True),\n","            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n","        )\n","\n","    def forward(self, out, y):\n","        \n","        y = y.view(-1, 256, 1, 1)\n","        y = y.repeat(1, 1, 4, 4)\n","        h_c_code = torch.cat((out, y), 1)\n","        out = self.joint_conv(h_c_code)\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7bDksNtt8ajY"},"source":["#**DISCRIMINATOR**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKP4xouxpADs"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, ndf):\n","        super(Discriminator, self).__init__()\n","\n","        self.conv_img = nn.Conv2d(3, ndf, 3, 1, 1)#128\n","        self.Downblock0 = DownBlock(ndf * 1, ndf * 2)#64\n","        self.Downblock1 = DownBlock(ndf * 2, ndf * 4)#32\n","        self.Downblock2 = DownBlock(ndf * 4, ndf * 8)#16\n","        self.Downblock3 = DownBlock(ndf * 8, ndf * 16)#8\n","        self.Downblock4 = DownBlock(ndf * 16, ndf * 16)#4\n","        self.Downblock5 = DownBlock(ndf * 16, ndf * 16)#4\n","\n","        self.COND_DNET = D_GET_LOGITS(ndf)\n","\n","    def forward(self,x):\n","\n","        out = self.conv_img(x)\n","        out = self.Downblock0(out)\n","        out = self.Downblock1(out)\n","        out = self.Downblock2(out)\n","        out = self.Downblock3(out)\n","        out = self.Downblock4(out)\n","        out = self.Downblock5(out)\n","\n","        return out\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LvwC5VEk82Vw"},"source":["#**DOWN BLOCKS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24KM5zb78zUs"},"outputs":[],"source":["class DownBlock(nn.Module):\n","    def __init__(self, fin, fout, downsample=True):\n","        super().__init__()\n","        self.downsample = downsample\n","        self.learned_shortcut = (fin != fout)\n","        self.conv_r = nn.Sequential(\n","            nn.Conv2d(fin, fout, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            nn.Conv2d(fout, fout, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","        )\n","\n","        self.conv_s = nn.Conv2d(fin,fout, 1, stride=1, padding=0)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x, c=None):\n","        return self.shortcut(x)+self.gamma*self.residual(x)\n","\n","    def shortcut(self, x):\n","        if self.learned_shortcut:\n","            x = self.conv_s(x)\n","        if self.downsample:\n","            return F.avg_pool2d(x, 2)\n","        return x\n","\n","    def residual(self, x):\n","        return self.conv_r(x)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Rrg4JQjy9A6j"},"source":["#**TESTING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4wq9HmWfW_5"},"outputs":[],"source":["def sampling(text_encoder, netG, dataloader,device):\n","    print(\"...............starting...................\")\n","    model_dir = cfg.TRAIN.NET_G\n","    split_dir = 'valid1'\n","    # Build and load the generator\n","    netG.load_state_dict(torch.load('/content/drive/MyDrive/DF-GAN/models/%s/netG.pth'%(cfg.CONFIG_NAME)))\n","    netG.eval()\n","\n","    batch_size = cfg.TRAIN.BATCH_SIZE\n","    s_tmp = model_dir\n","    save_dir = '%s/%s' % (s_tmp, split_dir)\n","    mkdir_p(save_dir)\n","    cnt = 0\n","    for i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n","        for step, data in enumerate(dataloader, 0):\n","            imags, captions, cap_lens, class_ids, keys = prepare_data(data)\n","            cnt += batch_size\n","            if step % 100 == 0:\n","                print('step: ', step)\n","            # if step > 50:\n","            #     break\n","            hidden = text_encoder.init_hidden(batch_size)\n","            # words_embs: batch_size x nef x seq_len\n","            # sent_emb: batch_size x nef\n","            words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n","            words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n","            #######################################################\n","            # (2) Generate fake images\n","            ######################################################\n","            with torch.no_grad():\n","                noise = torch.randn(batch_size, 100)\n","                noise=noise.to(device)\n","                fake_imgs = netG(noise,sent_emb)\n","            for j in range(batch_size):\n","                s_tmp = '%s/single/%s' % (save_dir, keys[j])\n","                folder = s_tmp[:s_tmp.rfind('/')]\n","                if not os.path.isdir(folder):\n","                    print('Make a new folder: ', folder)\n","                    mkdir_p(folder)\n","                im = fake_imgs[j].data.cpu().numpy()\n","                # [-1, 1] --> [0, 255]\n","                im = (im + 1.0) * 127.5\n","                im = im.astype(np.uint8)\n","                im = np.transpose(im, (1, 2, 0))\n","                im = Image.fromarray(im)\n","                fullpath = '%s_%3d.png' % (s_tmp,i)\n","                im.save(fullpath)\n","    print(\"....................ending................\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oi5qYjmc9F8E"},"source":["#**TRAINING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jY9gOPAGfUKG"},"outputs":[],"source":["def train(dataloader,netG,netD,text_encoder,optimizerG,optimizerD,state_epoch,batch_size,device):\n","    f=open(\"/content/drive/MyDrive/DF-GAN/code/epoch.txt\",'r')\n","    epochcount=0\n","    for i in f:\n","        epochcount=i\n","    f.close()\n","    netgpath=\"/content/drive/MyDrive/DF-GAN/models/bird/netG.pth\"\n","    netdpath=\"/content/drive/MyDrive/DF-GAN/models/bird/netD.pth\"\n","    netG.load_state_dict(torch.load(netgpath))\n","    netD.load_state_dict(torch.load(netdpath))\n","    for epoch in range(int(epochcount)+1, cfg.TRAIN.MAX_EPOCH+1):\n","        for step, data in enumerate(dataloader, 0):\n","            \n","            imags, captions, cap_lens, class_ids, keys = prepare_data(data)\n","            hidden = text_encoder.init_hidden(batch_size)\n","            # words_embs: batch_size x nef x seq_len\n","            # sent_emb: batch_size x nef\n","            words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n","            words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n","\n","            imgs=imags[0].to(device)\n","            real_features = netD(imgs)\n","            output = netD.COND_DNET(real_features,sent_emb)\n","            errD_real = torch.nn.ReLU()(1.0 - output).mean()\n","\n","            output = netD.COND_DNET(real_features[:(batch_size - 1)], sent_emb[1:batch_size])\n","            errD_mismatch = torch.nn.ReLU()(1.0 + output).mean()\n","\n","            # synthesize fake images\n","            noise = torch.randn(batch_size, 100)\n","            noise=noise.to(device)\n","            fake = netG(noise,sent_emb)  \n","            \n","            # G does not need update with D\n","            fake_features = netD(fake.detach()) \n","\n","            errD_fake = netD.COND_DNET(fake_features,sent_emb)\n","            errD_fake = torch.nn.ReLU()(1.0 + errD_fake).mean()          \n","\n","            errD = errD_real + (errD_fake + errD_mismatch)/2.0\n","            optimizerD.zero_grad()\n","            optimizerG.zero_grad()\n","            errD.backward()\n","            optimizerD.step()\n","\n","            #MA-GP\n","            interpolated = (imgs.data).requires_grad_()\n","            sent_inter = (sent_emb.data).requires_grad_()\n","            features = netD(interpolated)\n","            out = netD.COND_DNET(features,sent_inter)\n","            grads = torch.autograd.grad(outputs=out,\n","                                    inputs=(interpolated,sent_inter),\n","                                    grad_outputs=torch.ones(out.size()).cuda(),\n","                                    retain_graph=True,\n","                                    create_graph=True,\n","                                    only_inputs=True)\n","            grad0 = grads[0].view(grads[0].size(0), -1)\n","            grad1 = grads[1].view(grads[1].size(0), -1)\n","            grad = torch.cat((grad0,grad1),dim=1)                        \n","            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n","            d_loss_gp = torch.mean((grad_l2norm) ** 6)\n","            d_loss = 2.0 * d_loss_gp\n","            optimizerD.zero_grad()\n","            optimizerG.zero_grad()\n","            d_loss.backward()\n","            optimizerD.step()\n","            \n","            # update G\n","            features = netD(fake)\n","            output = netD.COND_DNET(features,sent_emb)\n","            errG = - output.mean()\n","            optimizerG.zero_grad()\n","            optimizerD.zero_grad()\n","            errG.backward()\n","            optimizerG.step()\n","\n","            print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f'\n","                % (epoch, cfg.TRAIN.MAX_EPOCH, step, len(dataloader), errD.item(), errG.item()))\n","        if(epoch%10==0):\n","            vutils.save_image(fake.data,\n","                        '%s/fake_samples_epoch_%03d.png' % ('/content/drive/MyDrive/DF-GAN/code/miscc/imgs/', epoch),\n","                        normalize=True)\n","\n","        if epoch%1==0:\n","            os.remove('/content/drive/MyDrive/DF-GAN/models/bird/netG.pth')\n","            os.remove('/content/drive/MyDrive/DF-GAN/models/bird/netD.pth')\n","            torch.save(netG.state_dict(), '/content/drive/MyDrive/DF-GAN/models/%s/netG.pth' % (cfg.CONFIG_NAME))\n","            torch.save(netD.state_dict(), '/content/drive/MyDrive/DF-GAN/models/%s/netD.pth' % (cfg.CONFIG_NAME))\n","            f=open(\"/content/drive/MyDrive/DF-GAN/code/epoch.txt\",'w')\n","            f.write(str(epoch))\n","            f.close()\n","        \n","    return 0"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"th6QrwsH9L27"},"source":["#**MAIN SECTION**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":978},"executionInfo":{"elapsed":19382,"status":"error","timestamp":1627576426421,"user":{"displayName":"Jagadeesh Nadimpalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiGtEyk2Ntf2Ry0318QvObdL70hIl4sqypFy_Z=s64","userId":"15245029944734342298"},"user_tz":-330},"id":"oExXw9VGixUK","outputId":"7e26686b-20e7-4ec4-ff34-85f7b960efb1"},"outputs":[],"source":["torch.cuda.set_device(cfg.GPU_ID)\n","cudnn.benchmark = True\n","\n","    # Get data loader ##################################################\n","imsize = cfg.TREE.BASE_SIZE\n","batch_size = cfg.TRAIN.BATCH_SIZE\n","image_transform = transforms.Compose([transforms.Resize(int(imsize * 76 / 64)),\n","                                      transforms.RandomCrop(imsize),\n","                                      transforms.RandomHorizontalFlip()])\n","if (cfg.B_VALIDATION):\n","  dataset = TextDataset(cfg.DATA_DIR, 'test',\n","                                base_size=cfg.TREE.BASE_SIZE,\n","                                transform=image_transform)\n","  print(dataset.n_words, dataset.embeddings_num)\n","  assert dataset\n","  dataloader = torch.utils.data.DataLoader(\n","            dataset, batch_size=batch_size, drop_last=True,\n","            shuffle=True, num_workers=int(cfg.WORKERS))\n","else:\n","  dataset = TextDataset(cfg.DATA_DIR, 'train',\n","                            base_size=cfg.TREE.BASE_SIZE,\n","                            transform=image_transform)\n","  print(dataset.n_words, dataset.embeddings_num)\n","  assert dataset\n","  dataloader = torch.utils.data.DataLoader(\n","            dataset, batch_size=batch_size, drop_last=True,\n","            shuffle=True, num_workers=int(cfg.WORKERS))\n","\n","    # # validation data #\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","netG = Generator(cfg.TRAIN.NF, 100).to(device)\n","netD = Discriminator(cfg.TRAIN.NF).to(device)\n","text_encoder = RNN_ENCODER(dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n","state_dict = torch.load(cfg.TEXT.DAMSM_NAME, map_location=lambda storage, loc: storage)\n","text_encoder.load_state_dict(state_dict)\n","text_encoder.cuda()\n","\n","for p in text_encoder.parameters():\n","  p.requires_grad = False\n","text_encoder.eval()    \n","\n","state_epoch=0\n","optimizerG = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\n","optimizerD = torch.optim.Adam(netD.parameters(), lr=0.0004, betas=(0.0, 0.9))  \n","\n","\n","if cfg.B_VALIDATION:\n","  count = sampling(text_encoder, netG, dataloader,device)  # generate images for the whole valid dataset\n","  print('state_epoch:  %d'%(state_epoch))\n","else:\n","  count = train(dataloader,netG,netD,text_encoder,optimizerG,optimizerD, state_epoch,batch_size,device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHfqKOXnixW8"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize\n","def cap(txt):\n","  token_text = sent_tokenize(txt)\n","  captions=[]\n","  for i in token_text:\n","    c=i.split()\n","    captions.append(c)\n","  return captions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kBGymTgsk1K"},"outputs":[],"source":["def imggen(txt):\n","  netG.load_state_dict(torch.load('/content/drive/MyDrive/DF-GAN/models/bird/netG.pth'))\n","  netG.eval()\n","  batch_size = cfg.TRAIN.BATCH_SIZE\n","  captions=torch.tensor(cap(txt))\n","  print(captions)\n","  cap_lens=len(captions)\n","  print(cap_lens)\n","  hidden=text_encoder.init_hidden(batch_size)\n","  words_embs,sent_emb=text_encoder(captions,cap_lens,hidden)\n","  words_embs,sent_emb=words_embs.detach(),sent_emb.detach()\n","  with torch.no_grad():\n","    noise=torch.randn(batch_size,100)\n","    noise=noise.to(device)\n","    fake_image=netG(noise,sent_emb)\n","  im=fake_image.data.cpu().numpy()\n","  im = (im + 1.0) * 127.5\n","  im = im.astype(np.uint8)\n","  im = np.transpose(im, (1, 2, 0))\n","  im = Image.fromarray(im)\n","  return im"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygvoWJkyixZz"},"outputs":[],"source":["from ipywidgets import widgets\n","label1=widgets.Label('Enter the text')\n","display(label1)\n","ta1=widgets.Textarea()\n","display(ta1)\n","btn=widgets.Button(description=\"dsp\")\n","display(btn)\n","label2=widgets.Label()\n","display(label2)\n","def dsp(b):\n","  #print(ta1.value)\n","  #x=ta1.value\n","  #y=x.split()\n","  #print(y)\n","  label2=imggen(ta1.value)\n","  #label2.value=ta1.value\n","btn.on_click(dsp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5PWa1ElixcE"},"outputs":[],"source":["text=\"\"\"the medium sized bird has a dark grey color, a black downward curved beak, and long wings.\n","        the bird is dark grey brown with a thick curved bill and a flat shaped tail.\n","        bird has brown body feathers, white breast feathers and black beak\n","        this bird has a dark brown overall body color, with a small white patch around the base of the bill.\n","        the bird has very long and large brown wings, as well as a black body and a long black beak.\n","        it is a type of albatross with black wings, tail, back and beak, and has a white ring at the base of its beak.\n","        this bird has brown plumage and a white ring at the base of its long, curved brown beak.\n","        the entire body is dark brown, as is the bill, with a white band encircling where the bill meets the head.\n","        this bird is gray in color, with a large curved beak.\n","        a large gray bird with a long wingspan and a long black beak.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvaRsnnefUMc"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize\n","token_text = sent_tokenize(text)\n","captions=[]\n","for i in token_text:\n","  print(i)\n","  c=i.split(' ')\n","  captions.append(c)\n","print(captions)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1625395300407,"user":{"displayName":"Jagadeesh Nadimpalli","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiGtEyk2Ntf2Ry0318QvObdL70hIl4sqypFy_Z=s64","userId":"15245029944734342298"},"user_tz":-330},"id":"6We2W9V-wQMt","outputId":"25491790-b5c8-4f51-fd01-7af75060f757"},"outputs":[],"source":["print(len(captions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEsk0h4vx3Ed"},"outputs":[],"source":["def load_captions(self, data_dir, filenames):\n","        all_captions = []\n","        for i in range(len(filenames)):\n","            cap_path = '%s/text/%s.txt' % (data_dir, filenames[i])\n","            with open(cap_path, \"r\") as f:\n","                captions = f.read().decode('utf8').split('\\n')\n","                cnt = 0\n","                for cap in captions:\n","                    if len(cap) == 0:\n","                        continue\n","                    cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n","                    # picks out sequences of alphanumeric characters as tokens\n","                    # and drops everything else\n","                    tokenizer = RegexpTokenizer(r'\\w+')\n","                    tokens = tokenizer.tokenize(cap.lower())\n","                    # print('tokens', tokens)\n","                    if len(tokens) == 0:\n","                        print('cap', cap)\n","                        continue\n","\n","                    tokens_new = []\n","                    for t in tokens:\n","                        t = t.encode('ascii', 'ignore').decode('ascii')\n","                        if len(t) > 0:\n","                            tokens_new.append(t)\n","                    all_captions.append(tokens_new)\n","                    cnt += 1\n","                    if cnt == self.embeddings_num:\n","                        break\n","                if cnt < self.embeddings_num:\n","                    print('ERROR: the captions for %s less than %d'\n","                          % (filenames[i], cnt))\n","        return all_captions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZBFnIc_HANm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"DFGAN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
