# 贝叶斯决策规则
## 最小错误率贝叶斯决策规则
$$
P\left(w_{i} \mid x\right)=\max P\left(w_{j} \mid x\right), \quad j \in[1, c]
$$
则 $x \in w_{i}$
因为对于每一类， $P(x)$ 都相等，所以
$$
P\left(x \mid w_{i}\right) P\left(w_{i}\right)=\max \left[P\left(x \mid w_{j}\right) P\left(w_{j}\right)\right]
$$
则 $x \in w_{i}$
分析错误率
$$
P(e)=\int_{-\infty}^{+\infty} P(\text { error }, x) d x=\int_{-\infty}^{+\infty} P(\operatorname{error} \mid x) P(x) d x
$$
$x$ 取不同值时，错误率的积分
$$
P(\text { error } \mid x)=\sum_{j=1}^{c} P\left(w_{j} \mid x\right)-\max P\left(w_{j} \mid x\right)
$$
## 最小风险贝叶斯决策规则
首先是期望风险
$$
\begin{aligned}
R_{\exp }(\alpha) &=\int_{\mathcal{X} \times \mathcal{Y}} L(y, \alpha(\mathbf{x})) P(y \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \mathrm{d} y \\
&=\int\left(\int L(y, \alpha(\mathbf{x})) P(y \mid \mathbf{x}) \mathrm{d} y\right) p(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&=\int\left(\sum_{y=1}^{c} L(y, \alpha(\mathbf{x})) P(y \mid \mathbf{x})\right) p(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&=\int R(\alpha(\mathbf{x}) \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x}
\end{aligned}
$$
由最后的
$$
R_{\text {exp }}(\alpha)=\int R(\alpha(\mathbf{x}) \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x}
$$
以及条件风险的概念: 将一个样本 $\mathrm{x}$ 分类为 $\omega_{i}$ 类的条件风险定义如下:
$$
R\left(\alpha_{i} \mid \mathbf{x}\right)=\sum_{j=1}^{c} \lambda_{i j} P\left(\omega_{j} \mid \mathbf{x}\right)
$$
得到
$$
R_{\exp }(\alpha)=\mathbb{E}[L(y, \alpha(\mathbf{x}))]=\mathbb{E}_{\mathbf{x}}[R(\alpha(\mathbf{x}) \mid \mathbf{x})]
$$
由此可得, 最小风险决策的决策规则为:
$$
\arg \min _{i} R\left(\alpha_{i} \mid \mathbf{x}\right)
$$
## 极小极大化准则
我们可以计算两类分类问题
$$
\begin{aligned}
&R\left(a_{1} \mid x\right)=\lambda_{11} \mathrm{P}\left(\omega_{1} \mid x\right)+\lambda_{12} \mathrm{P}\left(\omega_{2} \mid x\right) \\
&R\left(\mathrm{a}_{2} \mid {x}\right)=\lambda_{21} \mathrm{P}\left(\omega_{1} \mid {x}\right)+\lambda_{22} \mathrm{P}\left(\omega_{2} \mid {x}\right)
\end{aligned}
$$
各个式子的含义:
$R\left(a_{1} \mid {x}\right)$ 的意思是对特征值 ${x}$ 采取 $a_{1}$ 行动所可能导致的风险。
$R\left(a_{2} \mid x\right)$ 的意思是对特征值x采取 $a_{2}$ 行动所可能导致的风险。
$\lambda_{i j}$ 的含义是将 $\omega_{i}$ 类归到 $\omega_{j}$ 类所导致的错误代价。
按昭贝叶斯决策的意思，对特征值 $x$ 要选择风险最小的一个行为来action。也就是说如果 $R\left(a_{1} \mid {x}\right)<R\left(a_{2} \mid {x}\right)$ ，那么将特征值 $x$ 所在的样本判为 $\omega_{1}$ 类。
进一步演绎这个不等式，得
$$
\begin{aligned}
&\lambda_{11} \mathrm{P}\left(\omega_{1} \mid {x}\right)+\lambda_{12} \mathrm{P}\left(\omega_{2} \mid {x}\right)<\lambda_{21} \mathrm{P}\left(\omega_{1} \mid {x}\right)+\lambda_{22} \mathrm{P}\left(\omega_{2} \mid {x}\right) \\
&\lambda_{11} \mathrm{P}\left(\omega_{1} \mid {x}\right)-\lambda_{21} \mathrm{P}\left(\omega_{1} \mid {x}\right)<\lambda_{22} \mathrm{P}\left(\omega_{2} \mid {x}\right)-\lambda_{12} \mathrm{P}\left(\omega_{2} \mid {x}\right) \\
&\left(\lambda_{11}-\lambda_{21}\right) \mathrm{P}\left(\omega_{1} \mid {x}\right)<\left(\lambda_{22}-\lambda_{12}\right) \mathrm{P}\left(\omega_{2} \mid {x}\right)
\end{aligned}
$$
又根据贝叶斯公式
$$
P\left(\omega_{j} \mid x\right)=\frac{P\left(x \mid \omega_{j}\right) P\left(\omega_{j}\right)}{p(x)}
$$
可以整理为
$$\left(\lambda_{11}-\lambda_{21}\right) \mathrm{p}\left({x} \mid \omega_{1}\right) \mathrm{P}\left(\omega_{1}\right)<\left(\lambda_{22}-\lambda_{12}\right) \mathrm{p}\left({x} \mid \omega_{2}\right) \mathrm{P}\left(\omega_{2}\right)$$
写成分数形式为
$$\frac{p\left(x \mid \omega_{1}\right)}{p\left(x \mid \omega_{2}\right)}>\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}} \frac{P\left(\omega_{2}\right)}{P\left(\omega_{1}\right)}$$
若将不等式的右边用 $\theta$ 替代，即
$$\theta=\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}} \frac{P\left(\omega_{2}\right)}{P\left(\omega_{1}\right)}$$
那么将有
$$\frac{p\left(x \mid \omega_{1}\right)}{p\left(x \mid \omega_{2}\right)}>\theta$$

## 限定错误率
Neyman-Pearson 决策
梯度条件：决策边界满足
$$
\lambda=\frac{p\left(x \mid \omega_{1}\right)}{p\left(x \mid \omega_{2}\right)}, P_{2}(e)=\epsilon_{0}
$$
决策规则:
$$
\lambda p\left(x \mid \omega_{2}\right)-p\left(x \mid \omega_{1}\right)<0 \Rightarrow x \in \omega_{1}
$$
似然比:
$$
l(x)=\frac{p\left(x \mid \omega_{1}\right)}{p\left(x \mid \omega_{2}\right)}>\lambda \Rightarrow x \in \omega_{1}
$$
对偶变量求解: 通过 $l(x)$ 的映射关系，可由 $p(x)$ 求得 $p\left(l \mid \omega_{2}\right)$ ，则由定义可知误差率为
$$
\begin{aligned}
P_{2}(e) &=1-\int_{0}^{\lambda} p\left(l \mid \omega_{2}\right) \mathrm{d} l \\
&=\epsilon_{0} \Rightarrow \lambda
\end{aligned}
$$
# 贝叶斯决策分类器设计 
在去年郑州仓库的拯救者虚空神机中，拯救者Y9000K $\left(\omega_{1}\right)$ 和拯救者R7000P $\left(\omega_{2}\right)$ 的先验概率分布为 $P\left(\omega_{1}\right)=0.7, P\left(\omega_{2}\right)=0.3$, 是泡水机概率为 $P\left(X \mid \omega_{1}\right)=0.4, P\left(X \mid \omega_{2}\right)=0.5$, 假设损失 $\lambda_{11}=0, \lambda_{12}=1, \lambda_{21}=5, \lambda_{22}=0$，（一般人抢不到Y9000K，所以如果错误认为R7000P是泡水机损失更大）现在要识别一个泡水机是什么型号
## 按最小错误率规则
$$
\begin{aligned}
&P\left(\omega_{1} \mid X\right)=\frac{P\left(X \mid \omega_{1}\right) P\left(\omega_{1}\right)}{P\left(X \mid \omega_{1}\right) P\left(\omega_{1}\right)+P\left(X \mid \omega_{2}\right) P\left(\omega_{2}\right)}=\frac{0.4 \times 0.7}{0.4 \times 0.7+0.5 \times 0.3}=\frac{28}{43} \\
&P\left(w_{2} \mid X\right)=\frac{P\left(X \mid w_{2}\right) P\left(w_{2}\right)}{P\left(X \mid \omega_{1}\right) P(\omega)+P(X \mid \omega) P\left(\omega_{2}\right)}=\frac{0.5 \times 0.3}{0.4 \times 0.7+0.5 \times 0.3}=\frac{15}{43} \\
&P\left(\omega_{1} \mid X\right)>P\left(\omega_{2} \mid X\right) \quad \therefore x \in \omega_{1}
\end{aligned}
$$ 
## 按最小风险规则
$$
\begin{aligned}
&R_{1}(X)=\lambda_{11} P\left(X \mid \omega_{1}\right)+\lambda_{21} P\left(X \mid \omega_{2}\right)=0+2 \times \frac{28}{43}=\frac{56}{43} \\
&R_{2}(X)=\lambda_{12} P\left(X \mid \omega_{1}\right)+\lambda_{22} P\left(X \mid \omega_{2}\right)=1 \times \frac{15}{43}+0=\frac{15}{43} \\
&R_{1}(X)>R_{2}(X) \therefore x \in \omega_{2}
\end{aligned}
$$
（3）因为这里贝叶斯规则风险是常数，所以就是最小最大分类 
（4）限定错误率
